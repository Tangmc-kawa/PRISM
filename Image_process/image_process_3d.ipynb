{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mingchuan\\anaconda3\\envs\\cell-typing\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\Mingchuan\\anaconda3\\envs\\cell-typing\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\Mingchuan\\anaconda3\\envs\\cell-typing\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# from unittest.mock import patch\n",
    "\n",
    "from lib.utils.io_utils import get_tif_list\n",
    "from lib.fstack import stack_cyc\n",
    "from lib.cidre import cidre_correct, cidre_walk\n",
    "from lib.register import register_meta\n",
    "from lib.stitch import patch_tiles\n",
    "from lib.stitch import template_stitch\n",
    "from lib.stitch import stitch_offset\n",
    "\n",
    "from lib.register import register_manual\n",
    "from lib.stitch import stitch_manual\n",
    "from lib.os_snippets import try_mkdir\n",
    "    \n",
    "from skimage.io import imread\n",
    "from skimage.io import imsave\n",
    "from skimage.util import img_as_uint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "def resize_pad(img, size):\n",
    "    img_resized = resize(img, size, anti_aliasing=True)\n",
    "    img_padded = np.zeros(img.shape)\n",
    "    y_start, x_start = (img.shape[0] - size[0]) // 2, (img.shape[1] - size[1]) // 2\n",
    "    img_padded[y_start:y_start+size[0], x_start:x_start+size[1]] = img_resized\n",
    "    img_padded = img_as_uint(img_padded)\n",
    "    return img_padded\n",
    "\n",
    "\n",
    "def resize_dir(in_dir, out_dir, chn):\n",
    "    Path(out_dir).mkdir(exist_ok=True)\n",
    "    chn_sizes = {'cy3': 2302, 'TxRed': 2303, 'FAM': 2301, 'DAPI': 2300}\n",
    "    size = chn_sizes[chn]\n",
    "    im_list = list(Path(in_dir).glob(f'*.tif'))\n",
    "    for im_path in tqdm(im_list, desc=Path(in_dir).name):\n",
    "        im = imread(im_path)\n",
    "        im = resize_pad(im, (size, size))\n",
    "        imsave(Path(out_dir)/im_path.name, im, check_contrast=False)\n",
    "\n",
    "\n",
    "def resize_batch(in_dir, out_dir):\n",
    "    try_mkdir(out_dir)\n",
    "    cyc_paths = list(Path(in_dir).glob('cyc_*_*'))\n",
    "    for cyc_path in cyc_paths:\n",
    "        chn = cyc_path.name.split('_')[-1]\n",
    "        if chn == 'cy5':\n",
    "            shutil.copytree(cyc_path, Path(out_dir)/cyc_path.name)\n",
    "        else:\n",
    "            resize_dir(cyc_path, Path(out_dir)/cyc_path.name, chn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = Path(r'E:\\TMC\\spatial_data\\raw')\n",
    "BASE_DIR = Path(r'E:\\TMC\\spatial_data\\processed')\n",
    "RUN_ID = '20240418_100um_Gel_Actb_test2_new_sample'\n",
    "src_dir = SRC_DIR / RUN_ID\n",
    "dest_dir = BASE_DIR / f'{RUN_ID}_processed'\n",
    "\n",
    "# 2D workflow\n",
    "aif_dir = dest_dir / 'focal_stacked'\n",
    "sdc_dir = dest_dir / 'background_corrected'\n",
    "rgs_dir = dest_dir / 'registered'\n",
    "stc_dir = dest_dir / 'stitched'\n",
    "rsz_dir = dest_dir / 'resized'\n",
    "\n",
    "# 3D workflow\n",
    "cid_dir = dest_dir / 'cidre'\n",
    "air_dir = dest_dir / 'airlocalize_stack'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_2d():\n",
    "    # raw_cyc_list = list(src_dir.glob('cyc_*'))\n",
    "    # for cyc in raw_cyc_list:\n",
    "    #   cyc_num = int(cyc.name.split('_')[1])\n",
    "    #   stack_cyc(src_dir, aif_dir, cyc_num)\n",
    "\n",
    "    cidre_walk(str(aif_dir), str(sdc_dir))\n",
    "\n",
    "    rgs_dir.mkdir(exist_ok=True)\n",
    "    ref_cyc = 1\n",
    "    ref_chn = 'cy3'\n",
    "    ref_chn_1 = 'cy5'\n",
    "    ref_dir = sdc_dir / f'cyc_{ref_cyc}_{ref_chn}'\n",
    "    im_names = get_tif_list(ref_dir)\n",
    "\n",
    "    meta_df = register_meta(str(sdc_dir), str(rgs_dir), ['cy3', 'cy5'], im_names, ref_cyc, ref_chn)\n",
    "    meta_df.to_csv(rgs_dir / 'integer_offsets.csv')\n",
    "\n",
    "    # register_manual(rgs_dir/'cyc_1_cy3', sdc_dir/'cyc_1_cy5', rgs_dir/'cyc_1_cy5') #\n",
    "    register_manual(rgs_dir/'cyc_1_cy3', sdc_dir / 'cyc_1_FAM', rgs_dir/'cyc_1_FAM')\n",
    "    register_manual(rgs_dir/'cyc_1_cy3', sdc_dir / 'cyc_1_TxRed', rgs_dir/'cyc_1_TxRed')\n",
    "    register_manual(rgs_dir/'cyc_1_cy3', sdc_dir/'cyc_1_DAPI', rgs_dir/'cyc_1_DAPI')  # 0103 revised! Please remove this !\n",
    "    \n",
    "    patch_tiles(rgs_dir/f'cyc_{ref_cyc}_{ref_chn}', 28 * 22)\n",
    "\n",
    "    resize_batch(rgs_dir, rsz_dir)\n",
    "\n",
    "    stc_dir.mkdir(exist_ok=True)\n",
    "    template_stitch(rsz_dir/f'cyc_{ref_cyc}_{ref_chn_1}', stc_dir, 28, 22)\n",
    "    \n",
    "    # offset_df = pd.read_csv(rgs_dir / 'integer_offsets.csv', index_col=0)\n",
    "    # stitch_offset(rgs_dir, stc_dir, offset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your per-slice and per-stack programs\n",
    "def process_slice(slice_2d, channel): \n",
    "    if channel != 'cy5':\n",
    "        # resize and pad the slice\n",
    "        chn_sizes = {'cy3': 2302, 'txred': 2303, 'fam': 2301, 'dapi': 2300}\n",
    "        size = chn_sizes[channel]\n",
    "        slice_2d = resize_pad(slice_2d, (size, size))\n",
    "    return slice_2d  # Placeholder\n",
    "\n",
    "# Adjust shift_correction\n",
    "def shift_correction(signal_df, shift_df, meta_df, tile, cyc, ref_cyc=1):\n",
    "    adjusted_signals = []\n",
    "    file = f'FocalStack_{tile:03d}.tif'\n",
    "    for _, signal_row in signal_df.iterrows():    \n",
    "        local_x, local_y = signal_row['x_in_pix'], signal_row['y_in_pix']\n",
    "\n",
    "        # Apply shift if not reference cycle\n",
    "        if cyc != ref_cyc:\n",
    "            shift_entry = shift_df.loc[cyc, file]  # Assuming shift_df is indexed by cycle and file\n",
    "            y_shift, x_shift = map(int, shift_entry.split(' '))\n",
    "            current_x = local_x + x_shift\n",
    "            current_y = local_y + y_shift\n",
    "        else: current_x, currenty = local_x, local_y\n",
    "\n",
    "        adjusted_signals.append((current_x, current_y))\n",
    "\n",
    "    xy_adjusted = pd.DataFrame(adjusted_signals, columns=['x_in_pix', 'y_in_pix'])\n",
    "    signal_df[['x_in_pix', 'y_in_pix']] = xy_adjusted[['x_in_pix', 'y_in_pix']]\n",
    "    \n",
    "    return signal_df\n",
    "\n",
    "def stitch_3d(signal_df, meta_df, tile):\n",
    "    adjusted_signals = []\n",
    "    file = f'FocalStack_{tile:03d}.tif'\n",
    "    # Find the metadata row for this file to get its global position\n",
    "    for _, signal_row in signal_df.iterrows(): \n",
    "        meta_row = meta_df.loc[meta_df['file'] == file].iloc[0]\n",
    "        local_x, local_y = signal_row['x_in_pix'], signal_row['y_in_pix']\n",
    "        global_x_start, global_y_start = meta_row['x'], meta_row['y']\n",
    "        global_x = global_x_start + local_x\n",
    "        global_y = global_y_start + local_y\n",
    "        adjusted_signals.append((global_x, global_y))\n",
    "\n",
    "    xy_adjusted = pd.DataFrame(adjusted_signals, columns=['x_in_pix', 'y_in_pix'])\n",
    "    signal_df[['x_in_pix', 'y_in_pix']] = xy_adjusted[['x_in_pix', 'y_in_pix']]\n",
    "    return signal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from zmq import CHANNEL\n",
    "\n",
    "import tifffile\n",
    "from lib.stitch import read_meta\n",
    "from lib.AIRLOCALIZE.airlocalize import airlocalize\n",
    "\n",
    "\n",
    "extract_points_cycle = ['C001']\n",
    "CHANNELS = ['cy3', 'cy5', 'fam', 'txred']\n",
    "\n",
    "def process_3d():\n",
    "\n",
    "    # generate corrected 3d image of each tile\n",
    "    # cidre_correct(str(src_dir), str(cid_dir))\n",
    "    stack_name = dict()\n",
    "    file_groups = defaultdict(list)\n",
    "    for file_path in glob.glob(os.path.join(src_dir, '*.tif')):\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split('-')\n",
    "        cycle, tile, channel = parts[0], parts[1], parts[2]\n",
    "        z_index = int(filename.split('Z')[-1].split('.')[0])\n",
    "        file_groups[(cycle, tile, channel)].append((z_index, file_path))\n",
    "        if tile in stack_name: stack_name[tile].append(cycle)\n",
    "        else: stack_name[tile] = [cycle]\n",
    "\n",
    "    stack_name = {key: sorted(value, key=lambda x: int(x[1:])) for key, value in stack_name.items()}\n",
    "    file_groups = {k: sorted(v) for k, v in file_groups.items()}  # Sort by Z index within each group\n",
    "\n",
    "    for (cycle, tile, channel), files in tqdm(file_groups.items(), desc='Processing stacks'):\n",
    "        stack = np.array([process_slice(imread(file_path), channel) for _, file_path in files])\n",
    "        os.makedirs(air_dir / tile / cycle, exist_ok=True)\n",
    "        imsave(air_dir / tile / cycle / f\"{channel.lower()}.tif\", stack)\n",
    "\n",
    "\n",
    "    # extract spot candidates from cyc1-4\n",
    "    for tile in tqdm(stack_name.keys(), desc='Detecting candidate points'):\n",
    "        for cycle in stack_name[tile]:\n",
    "            if cycle in extract_points_cycle: \n",
    "                tile_cycle_dir = air_dir  / tile / cycle\n",
    "                # perform airlocalization\n",
    "                airlocalize(parameters_filename='Image_process/lib/AIRLOCALIZE/parameters.yaml', \n",
    "                            default_parameters='Image_process/lib/AIRLOCALIZE/parameters_default.yaml',\n",
    "                            update={'dataFileName': tile_cycle_dir, 'saveDirName': tile_cycle_dir, 'verbose':False, 'multiChannelCandidates': True})\n",
    "                \n",
    "                spots_file = [_ for _ in os.listdir(tile_cycle_dir) if _.endswith('spots.csv')]\n",
    "                if 'intensity_local.csv' in os.listdir(air_dir/tile):\n",
    "                    df = pd.read_csv(air_dir / tile / 'intensity_local.csv')\n",
    "                    df = pd.concat([df] + [pd.read_csv(tile_cycle_dir / file) for file in spots_file], axis=1)\n",
    "                else: df = pd.concat([pd.read_csv(tile_cycle_dir / file) for file in spots_file], axis=1)\n",
    "                df.to_csv(air_dir / tile / 'intensity_local.csv', index=False)\n",
    "\n",
    "\n",
    "    # # multi-channel read\n",
    "    # shift_df = pd.read_csv(rgs_dir / 'integer_offsets.csv', index_col=0)\n",
    "\n",
    "    # for tile in tqdm(stack_name.keys(), desc='Reading spots'):\n",
    "    #     combined_candidates = pd.read_csv(air_dir / tile / 'combined_candidates.csv')\n",
    "    #     intensity_read = combined_candidates[['z_in_pix', 'x_in_pix', 'y_in_pix']].round().astype(np.uint16).drop_duplicates()\n",
    "    #     intensity_read = intensity_read.reset_index()\n",
    "\n",
    "    #     for cycle in stack_name[tile]:\n",
    "    #         with tifffile.TiffFile(air_dir / tile / 'cy3.tif') as tif:\n",
    "    #             shape = tif.series[0].shape\n",
    "\n",
    "    #         coordinates = intensity_read[['z_in_pix', 'x_in_pix', 'y_in_pix']]\n",
    "    #         coordinates = shift_correction(coordinates, shift_df, cyc=int(cycle[1:]), ref_cyc=1)\n",
    "    #         coordinates = coordinates[\n",
    "    #             (0 <= coordinates['x_in_pix'] < shape[1]) &\n",
    "    #             (0 <= coordinates['y_in_pix'] < shape[2]) &\n",
    "    #             (0 <= coordinates['z_in_pix'] < shape[0]) ]\n",
    "    #         z_coords = coordinates['z_in_pix'].to_numpy()\n",
    "    #         y_coords = coordinates['y_in_pix'].to_numpy()\n",
    "    #         x_coords = coordinates['x_in_pix'].to_numpy()\n",
    "\n",
    "    #         for channel in CHANNELS:\n",
    "    #             image = imread(air_dir/tile/f'{channel}.tif')\n",
    "    #             coordinates[f'{cycle}_{channel}'] = image[z_coords, y_coords, x_coords]\n",
    "    #             intensity_read['cyc{}_{}'.format(int(cycle[1:]), channel)] = coordinates[f'{cycle}_{channel}']\n",
    "\n",
    "    #     intensity_read.to_csv(aif_dir / tile / 'intensity_local.csv')\n",
    "    \n",
    "\n",
    "    # stitch the intensity\n",
    "    meta_df = read_meta(stc_dir)\n",
    "    pattern = r'\\((\\d+)\\, *(\\d+)\\)'\n",
    "    meta_df['match'] = meta_df['position'].apply(lambda x: re.match(pattern, x))\n",
    "    meta_df['y'] = meta_df['match'].apply(lambda x: int(x.group(2)))\n",
    "    meta_df['x'] = meta_df['match'].apply(lambda x: int(x.group(1)))\n",
    "    \n",
    "    intensity = None\n",
    "    for tile in tqdm(stack_name.keys(), desc='Stitching'):\n",
    "        signal_df = pd.read_csv(aif_dir / tile / 'intensity_local.csv')\n",
    "        if intensity is None: intensity = stitch_3d(signal_df, meta_df, tile)\n",
    "        else: intensity = pd.concat([intensity, stitch_3d(signal_df, meta_df, tile)])\n",
    "\n",
    "    intensity.to_csv(air_dir / 'intensity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     process_2d()\n",
    "#     process_3d()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting candidate points:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to analyze.\n",
      "==================================================\n",
      "Analyzing file: cy5.tif...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 226.0 to 2288.0\n",
      "Smoothing cy5.tif, mode: LoG...\n",
      "Image scaled from -2725.379194488983 to 5014.253662665929\n",
      "Image scaled from 0.0 to 32767.0\n",
      "Smoothing cy5.tif done.\n",
      "Threshold value is 14548.717142145391 in absolute units\n",
      "Predetected 34996 spots;\n",
      "Found 34996 spot candidates.\n",
      "==================================================\n",
      "Localizing spots...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 226.0 to 2288.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit predetected spots in cy5.tif: 100%|██████████| 34996/34996 [01:26<00:00, 404.89it/s]\n",
      "Detecting candidate points:  25%|██▌       | 1/4 [09:51<29:35, 591.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to analyze.\n",
      "==================================================\n",
      "Analyzing file: cy5.tif...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 218.0 to 3237.0\n",
      "Smoothing cy5.tif, mode: LoG...\n",
      "Image scaled from -2766.4469706295135 to 5072.422666331294\n",
      "Image scaled from 0.0 to 32767.0\n",
      "Smoothing cy5.tif done.\n",
      "Threshold value is 14572.554711848476 in absolute units\n",
      "Predetected 36500 spots;\n",
      "Found 36500 spot candidates.\n",
      "==================================================\n",
      "Localizing spots...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 218.0 to 3237.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit predetected spots in cy5.tif: 100%|██████████| 36500/36500 [01:27<00:00, 416.04it/s]\n",
      "Detecting candidate points:  50%|█████     | 2/4 [19:40<19:40, 590.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to analyze.\n",
      "==================================================\n",
      "Analyzing file: cy5.tif...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 177.0 to 1073.0\n",
      "Smoothing cy5.tif, mode: LoG...\n",
      "Image scaled from -2996.9531597865084 to 4396.179600600583\n",
      "Image scaled from 0.0 to 32767.0\n",
      "Smoothing cy5.tif done.\n",
      "Threshold value is 16194.135210149994 in absolute units\n",
      "Predetected 34248 spots;\n",
      "Found 34248 spot candidates.\n",
      "==================================================\n",
      "Localizing spots...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 177.0 to 1073.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit predetected spots in cy5.tif: 100%|██████████| 34248/34248 [01:27<00:00, 392.63it/s]\n",
      "Detecting candidate points:  75%|███████▌  | 3/4 [29:21<09:45, 585.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files to analyze.\n",
      "==================================================\n",
      "Analyzing file: cy5.tif...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 194.0 to 2902.0\n",
      "Smoothing cy5.tif, mode: LoG...\n",
      "Image scaled from -2845.862285801606 to 4580.24960915822\n",
      "Image scaled from 0.0 to 32767.0\n",
      "Smoothing cy5.tif done.\n",
      "Threshold value is 15143.907423608096 in absolute units\n",
      "Predetected 34922 spots;\n",
      "Found 34922 spot candidates.\n",
      "==================================================\n",
      "Localizing spots...\n",
      "Retrieving image for cy5.tif...\n",
      "Image scaled from 194.0 to 2902.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fit predetected spots in cy5.tif: 100%|██████████| 34922/34922 [01:20<00:00, 432.52it/s]\n",
      "Detecting candidate points: 100%|██████████| 4/4 [38:49<00:00, 582.33s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from zmq import CHANNEL\n",
    "\n",
    "import tifffile\n",
    "from lib.stitch import read_meta\n",
    "from lib.AIRLOCALIZE.airlocalize import airlocalize\n",
    "\n",
    "extract_points_cycle = ['C001']\n",
    "def process_3d():\n",
    "    # cidre_correct(str(src_dir), str(cid_dir))\n",
    "    stack_name = dict()\n",
    "    file_groups = defaultdict(list)\n",
    "    for file_path in glob.glob(str(src_dir / 'cyc_1_tile_stitch_test' / '*.tif')):\n",
    "        filename = os.path.basename(file_path)\n",
    "        parts = filename.split('-')\n",
    "        cycle, tile, channel = parts[0], parts[1], parts[2]\n",
    "        z_index = int(filename.split('Z')[-1].split('.')[0])\n",
    "        file_groups[(cycle, tile, channel)].append((z_index, file_path))\n",
    "        if tile in stack_name: stack_name[tile].add(cycle)\n",
    "        else: stack_name[tile] = {cycle}\n",
    "\n",
    "    stack_name = {key: sorted(value, key=lambda x: int(x[1:])) for key, value in stack_name.items()}\n",
    "    file_groups = {k: sorted(v) for k, v in file_groups.items()}  # Sort by Z index within each group\n",
    "\n",
    "    # for (cycle, tile, channel), files in tqdm(file_groups.items(), desc='Processing stacks'):\n",
    "    #     stack = np.array([process_slice(imread(file_path), channel) for _, file_path in files])\n",
    "    #     os.makedirs(air_dir / tile / cycle, exist_ok=True)\n",
    "    #     imsave(air_dir / tile / cycle / f\"{channel.lower()}.tif\", stack, check_contrast=False)\n",
    "\n",
    "\n",
    "    # extract spot candidates from cyc1-4\n",
    "    for tile in tqdm(stack_name.keys(), desc='Detecting candidate points'):\n",
    "        for cycle in stack_name[tile]:\n",
    "            if cycle in extract_points_cycle: \n",
    "                tile_cycle_dir = air_dir  / tile / cycle\n",
    "                # perform airlocalization\n",
    "                airlocalize(parameters_filename='./lib/AIRLOCALIZE/parameters.yaml', \n",
    "                            default_parameters='./lib/AIRLOCALIZE/parameters_default.yaml', \n",
    "                            update={'dataFileName': tile_cycle_dir, 'saveDirName': tile_cycle_dir, 'verbose':True, 'multiChannelCandidates': True})\n",
    "                \n",
    "                spots_file = [_ for _ in os.listdir(tile_cycle_dir) if _.endswith('spots.csv')]\n",
    "                if 'intensity_local.csv' in os.listdir(air_dir/tile):\n",
    "                    df = pd.read_csv(air_dir / tile / 'intensity_local.csv')\n",
    "                    df = pd.concat([df] + [pd.read_csv(tile_cycle_dir / file) for file in spots_file], axis=1)\n",
    "                else: df = pd.concat([pd.read_csv(tile_cycle_dir / file) for file in spots_file], axis=1)\n",
    "                df.to_csv(air_dir / tile / 'intensity_local.csv', index=False)\n",
    "\n",
    "process_3d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
